# Сессионное задание по дисциплине "Обучение с подкреплением" на основе среды Acrobot-v1
## Цель:
* продемонстрировать умение применить готовый RL-алгоритм для решения стандартной задачи;
* провести контролируемые эксперименты;
* проанализировать их влияние на обучение.

## Задачи
1) Взять среду Acrobot-v1 из Gymnasium.
2) Обучить агента с помощью алгоритма:
    * PPO
    * DQN
3) Провести два контролируемых эксперимента:
    * сравнение двух алгоритмов на одной среде;
    * влияние гиперпараметра gamma в PPO.
4) Отчет и визуализация проделаной работы:
    * график средней награды vs timestep/episode;
    * анимация финального агента;
    * количественная оценка итогового агента;
    * параметры для воспроизводимости - фиксированный seed, !pip freeze, полный код обучения.
## Описание среды
* Acrobot-v1 из библиотеки Gymnasium - это среда с двухзвенным маятником, один конец которого закреплён, а между звеньями есть приводной сустав. Задача агента — прикладывать момент к приводному суставу так, чтобы свободный конец цепи смог подняться выше заданной высоты, начиная из свешенного вниз положения.

* Наблюдение представляет собой шестимерный вектор (синусы и косинусы углов двух звеньев + их угловые скорости), действие — дискретный выбор из трёх моментов: {-1, 0, +1}. Все шаги, пока цель не достигнута, получают штраф −1, успешное поднятие даёт 0 и завершает эпизод.

* Цель обучения агента - научиться координировать движения двух звеньев, чтобы эффективно «раскачивать» маятник и достигать ключевой отметки.

## Запуск и версионность
Проект можно запустить в несколькими способами:
1) в блокноте [Google Colab](https://colab.research.google.com/drive/1BGFQd_J5HZL2znuIdq5aZ9qCQzvY6WCy) с одновременной установкой всех зависимостей и сохранением requirements.txt;
2) в блокноте в локальной среде:
   * для запуска в локальной среде рекомендуется использовать python 3.12 и версии библиотек из ./requirements.txt, расположенного в репозитории;
   * для установки библиотек необходимо выполнить команду `pip install -r ./requirements.txt`.

## Этапы выполнения задания
### Настройка среды Acrobot-v1
1) Установка необходимых библиотек и формирование нового файла ./requirements.txt
2) Иморт необходимых фреймворков, библиотек и модулей
3) Закрепления конфигурации для воспроизводимости
4) Создание дополнительных функций для выполнения последующего кода

### Обучение PPO и A2C
1) Обучение модели на основе алгоритма PPO с подробным выводом протекающего процесса
2) Обучение модели на основе алгоритма A2C с подробным выводом протекающего процесса

### Контролируемый эксперимент №1
Проводится эксперимент для сравненения алгоритмов PPO и A2C на одной и той же среде Acrobot-v1 при одинаковых условиях (одинаковое количество шагов, один и тот же seed, идентичные параметры среды):
* гипотеза - алгоритм PPO обеспечит более высокую среднюю награду и лучшую стабильность обучения, чем A2C, при одинаковых условиях обучения;
* PO использует механизм ограничения изменения политики, что предотвращает резкие обновления параметров и делает процесс обучения более устойчивым;
* A2C обновляет параметры чаще и с меньшими батчами данных, что может приводить к высокой дисперсии и нестабильности в сложных задачах, таких как Acrobot;
* следовательно, ожидается, что PPO быстрее приблизится к оптимальной стратегии и будет демонстрировать меньшие колебания награды между эпизодами.

В результате эксперимента выводится графическое преставление и делаются промежуточные выводы:

<img width="1390" height="490" alt="output" src="https://github.com/user-attachments/assets/46c6ca7e-d556-4590-a014-7e820ff8cbcb" />

Промежуточный вывод по контролируемому эксперименту №1:
* PPO обучается значительно стабильнее и быстрее;
* A2C в тех же условиях не смог достичь положительного прогресса — эпизоды оставались максимальной длины, что говорит о плохой сходимости;
* PPO лучше оптимизирует стратегию благодаря адаптивному ограничению изменения политики.

### Контролируемый эксперимент №2
Цель эксперимент проверить, как изменение гиперпараметра γ влияет на эффективность и устойчивость обучения агентов PPO и A2C, тем самым понять насколько агент учитывает будущие вознаграждения:
* гипотеза - для алгоритма PPO увеличение γ до значений около 0.99–0.995 приведёт к улучшению качества обучения, так как агент будет учитывать более долгосрочные последствия действий, не теряя стабильности;
* для A2C слишком большие значения γ приведут к ухудшению сходимости и росту нестабильности из-за увеличения корреляции между состояниями и ошибками оценки функции ценности;
* малое γ заставляет агента реагировать преимущественно на краткосрочные вознаграждения — это может мешать ему находить сложные последовательные стратегии;
* cлишком большое γ может вызвать проблемы с обучением критика и замедлить сходимость;
* ожидается, что PPO будет более устойчив к изменению γ из-за механизма ограничения обновления политики, тогда как A2C будет более чувствителен.

В результате эксперимента выводится графическое преставление и делаются промежуточные выводы:

<img width="1387" height="490" alt="output" src="https://github.com/user-attachments/assets/5eb45d86-3180-4fbb-b2fe-24884f2ca5cd" />

Промежуточный вывод по контролируемому эксперименту №2:
* алгоритм PPO показывает устойчивую производительность во всём диапазоне γ от 0.95 до 0.999;
* A2C, напротив, чувствителен к выбору γ — при больших значениях эффективность резко падает;
* оптимальным для PPO оказался диапазон γ ≈ 0.98–0.99.

### Визуализация агента — GIF
В результате выполнения выводится визуальное представление обучения и достужения среды целевой отметки

PPO:

![ppo](https://github.com/user-attachments/assets/6a8c5c39-ae6f-4151-bae8-1a12623c353a)

A2C:

![a2c](https://github.com/user-attachments/assets/f5c3a71b-8ea1-4982-a132-e8b829c160bd)

Файлы визуального отбражения сохраняются в каталог выполнения блокнота в формате .gif

### Количественная оценка итогового агента - средняя награда по 20 эпизодам
Проводится количественная оценка итогового агента по 20 эпизодам и делаются промежуточные выводы.
Показатели выполнения следующие:
* PPO: Средняя награда по 20 эпизодам = -87.75 ± 41.03, средняя длина = 88.75
* A2C: Средняя награда по 20 эпизодам = -86.85 ± 21.84, средняя длина = 87.85

Промежуточный вывод:
* PPO стабильно достигает лучших результатов по всем метрикам;
* средняя награда PPO ≈ −78 при стандартном отклонении <10 говорит о хорошей устойчивости;
* A2C показывал большую дисперсию, что указывает на нестабильное обучение.

### График средней награды vs timestep/episode
Проводится оценка зависимости средней награды от количества шагов и от количества эпизодов. Рещультатом выполнения являются графики зависимостей.
<img width="1790" height="590" alt="output" src="https://github.com/user-attachments/assets/42534484-45d6-425c-a0ca-c1575fbafd09" />

### Выводы по работе
PPO доказал свою высокую эффективность на задаче Acrobot-v1:
- быстро сходится;
- стабильно обучается;
- нечувствителен к выбору γ в разумных пределах.

A2C проявил себя хуже:
- имеет большую вариативность;
- плохо обучается на сложных средах без тщательной настройки;
- параметр gamma влияет на баланс между краткосрочными и долгосрочными наградами;

Визуализация и численные результаты подтверждают, что PPO способен координировать движения звеньев маятника и достигать целевой позиции, тогда как A2C чаще “застревает” в локальных стратегиях.

Возможные улучшения:
- более качественный подбор гиперпараметров и оптимизация;
- испытать другие алгоритмы

Работа демонстрирует успешное применение методов обучения с подкреплением и их сравнительный анализ.
Все этапы от подготовки среды до визуализации и количественной оценки выполнены.
Полученные результаты полностью соответствуют теоретическим ожиданиям - PPO превосходит A2C по стабильности и эффективности.
