# Сессионное задание по дисциплине "Обучение с подкреплением" на основе среды Acrobot-v1
## Цель:
* продемонстрировать умение применить готовый RL-алгоритм для решения стандартной задачи;
* провести контролируемые эксперименты;
* проанализировать их влияние на обучение.

## Задачи
1) Взять среду Acrobot-v1 из Gymnasium.
2) Обучить агента с помощью алгоритма:
    * PPO
    * DQN
3) Провести два контролируемых эксперимента:
    * сравнение двух алгоритмов на одной среде;
    * влияние гиперпараметра gamma в PPO.
4) Отчет и визуализация проделаной работы:
    * график средней награды vs timestep/episode;
    * анимация финального агента;
    * количественная оценка итогового агента;
    * параметры для воспроизводимости - фиксированный seed, !pip freeze, полный код обучения.
## Описание среды
* Acrobot-v1 из библиотеки Gymnasium - это среда с двухзвенным маятником, один конец которого закреплён, а между звеньями есть приводной сустав. Задача агента — прикладывать момент к приводному суставу так, чтобы свободный конец цепи смог подняться выше заданной высоты, начиная из свешенного вниз положения.

* Наблюдение представляет собой шестимерный вектор (синусы и косинусы углов двух звеньев + их угловые скорости), действие — дискретный выбор из трёх моментов: {-1, 0, +1}. Все шаги, пока цель не достигнута, получают штраф −1, успешное поднятие даёт 0 и завершает эпизод.

* Цель обучения агента - научиться координировать движения двух звеньев, чтобы эффективно «раскачивать» маятник и достигать ключевой отметки.

## Запуск и версионность
Проект можно запустить в несколькими способами:
1) в блокноте [Google Colab](https://colab.research.google.com/drive/1OpaR-dcs2n6UUVLNYQnM1uW2eQM1yXPp) с одновременной установкой всех зависимостей и сохранением requirements.txt;
2) в блокноте в локальной среде:
   * для запуска в локальной среде рекомендуется использовать python 3.12 и версии библиотек из ./requirements.txt, расположенного в репозитории;
   * для установки библиотек необходимо выполнить команду `pip install -r ./requirements.txt`.

## Этапы выполнения задания
### Настройка среды Acrobot-v1
1) Установка необходимых библиотек и формирование нового файла ./requirements.txt
2) Иморт необходимых фреймворков, библиотек и модулей
3) Закрепления конфигурации для воспроизводимости
4) Создание дополнительных функций для выполнения последующего кода

### Обучение PPO и A2C
1) Обучение модели на основе алгоритма PPO с подробным выводом протекающего процесса
2) Обучение модели на основе алгоритма A2C с подробным выводом протекающего процесса

### Контролируемый эксперимент №1
Проводится эксперимент для сравненения алгоритмов PPO и A2C на одной и той же среде Acrobot-v1 при одинаковых условиях (одинаковое количество шагов, один и тот же seed, идентичные параметры среды):
* гипотеза - алгоритм PPO обеспечит более высокую среднюю награду и лучшую стабильность обучения, чем A2C, при одинаковых условиях обучения;
* PO использует механизм ограничения изменения политики, что предотвращает резкие обновления параметров и делает процесс обучения более устойчивым;
* A2C обновляет параметры чаще и с меньшими батчами данных, что может приводить к высокой дисперсии и нестабильности в сложных задачах, таких как Acrobot;
* следовательно, ожидается, что PPO быстрее приблизится к оптимальной стратегии и будет демонстрировать меньшие колебания награды между эпизодами.

В результате эксперимента выводится графическое преставление и делаются промежуточные выводы:

<img width="1390" height="490" alt="output" src="https://github.com/user-attachments/assets/fb201951-d297-4fc0-92f8-a65b6010d231" />

Промежуточный вывод по контролируемому эксперименту №1:
* PPO обучается значительно стабильнее и быстрее;
* A2C в тех же условиях не смог достичь положительного прогресса — эпизоды оставались максимальной длины, что говорит о плохой сходимости;
* PPO лучше оптимизирует стратегию благодаря адаптивному ограничению изменения политики.

